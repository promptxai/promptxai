{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "769230c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptxai import models, content, prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74482f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai = models.OpenAI()\n",
    "model_card = content.ModelCard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b663a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_card.set_url('Wikipedia', 'https://en.wikipedia.org/wiki/GPT-3')\n",
    "model_card.set_url('GitHub', 'https://github.com/openai/gpt-3')\n",
    "model_card.set_url('Research Paper', 'https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b60d5e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = content.Webpage(model_card.get_url('Wikipedia'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1c9be6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generative Pre-trained Transformer 3 (GPT-3) is an autoregressive language model released in 2020 that uses deep learning to produce human-like text. Given an initial text as prompt, it will produce text that continues the prompt.\n",
      " The architecture is a decoder-only transformer network with a 2048-token-long context and then-unprecedented size of 175 billion parameters, requiring 800GB to store. The model was trained using generative pre-training; it is trained to predict what the next token is based on previous tokens. The model demonstrated strong zero-shot and few-shot learning on many tasks.[2] The authors described how language understanding performances in natural language processing (NLP) were improved in GPT-n through a process of \"generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative fine-tuning on each specific task.\" This eliminated the need for human supervision and for time-intensive hand-labeling.[2]\n",
      " It is the third-generation language prediction model in the GPT series, successor to GPT-2 created by OpenAI, a San Francisco-based artificial intelligence research laboratory.[3] GPT-3, which was introduced in May 2020, and was in beta testing as of July 2020,[4] is part of a trend in natural language processing (NLP) systems of pre-trained language representations.[1]\n",
      " The quality of the text generated by GPT-3 is so high that it can be difficult to determine whether or not it was written by a human, which has both benefits and risks.[5] Thirty-one OpenAI researchers and engineers presented the original May 28, 2020 paper introducing GPT-3. In their paper, they warned of GPT-3's potential dangers and called for research to mitigate risk.[1]:‚Ää34‚Ää David Chalmers, an Australian philosopher, described GPT-3 as \"one of the most interesting and important AI systems ever produced.\"[6] An April 2022 review in The New York Times described GPT-3's capabilities as being able to write original prose with fluency equivalent to that of a human.[7]\n",
      " Microsoft announced on September 22, 2020, that it had licensed \"exclusive\" use of GPT-3; others can still use the public API to receive output, but only Microsoft has access to GPT-3's underlying model.[8]\n",
      " According to The Economist, improved algorithms, powerful computers, and an increase in digitized data have fueled a revolution in machine learning, with new techniques in the 2010s resulting in \"rapid improvements in tasks\" including manipulating language.[9] Software models are trained to learn by using thousands or millions of examples in a \"structure¬†... loosely based on the neural architecture of the brain\".[9] One architecture used in natural language processing (NLP) is a neural network based on a deep learning model that was first introduced in 2017‚Äîthe Transformer.[10] GPT-n models are Transformer-based deep learning neural network architectures. There are a number of NLP systems capable of processing, mining, organizing, connecting and contrasting textual input, as well as correctly answering questions.[11]\n",
      " On June 11, 2018, OpenAI researchers and engineers posted their original paper on generative models‚Äîlanguage models‚Äîartificial intelligence systems‚Äîthat could be pre-trained with an enormous and diverse corpus of text via datasets, in a process they called generative pre-training (GP).[2] The authors described how language understanding performances in natural language processing (NLP) were improved in GPT-n through a process of \"generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative fine-tuning on each specific task.\" This eliminated the need for human supervision and for time-intensive hand-labeling.[2]\n",
      " In February 2020, Microsoft introduced its Turing Natural Language Generation (T-NLG), which was claimed to be the \"largest language model ever published at 17 billion parameters.\"[12] It performed better than any other language model at a variety of tasks which included summarizing texts and answering questions.\n",
      " The construct of ‚Äúlearning styles‚Äù is problematic because it fails to account for the processes through which learning styles are shaped. Some students might develop a particular learning style because they have had particular experiences. Others might develop a particular learning style by trying to accommodate to a learning environment that was not well suited to their learning needs. Ultimately, we need to understand the interactions among learning styles and environmental and personal factors, and how these shape how we learn and the kinds of learning we experience.\n",
      " ‚Äì Text generated by Mike Sharples[13]\n",
      " On May 28, 2020, an arXiv preprint by a group of 31 engineers and researchers at OpenAI described the development of GPT-3, a third-generation \"state-of-the-art language model\".[1][5] The team increased the capacity of GPT-3 by over two orders of magnitude from that of its predecessor, GPT-2,[14] making GPT-3 the largest non-sparse language model to date.[1]:‚Ää14‚Ää[3] Because GPT-3 is structurally similar to its predecessors,[1] its greater accuracy is attributed to its increased capacity and greater number of parameters.[15] GPT-3's capacity is ten times larger than that of Microsoft's Turing NLG, the next largest NLP model known at the time.[5]\n",
      " Sixty percent of the weighted pre-training dataset for GPT-3 comes from a filtered version of Common Crawl consisting of 410 billion byte-pair-encoded tokens.[1]:‚Ää9‚Ää Other sources are 19 billion tokens from WebText2 representing 22% of the weighted total, 12 billion tokens from Books1 representing 8%, 55 billion tokens from Books2 representing 8%, and 3 billion tokens from Wikipedia representing 3%.[1]:‚Ää9‚Ää GPT-3 was trained on hundreds of billions of words and is also capable of coding in CSS, JSX, and Python, among others.[4]\n",
      " Since GPT-3's training data was all-encompassing, it does not require further training for distinct language tasks.[4] The training data contains occasional toxic language and GPT-3 occasionally generates toxic language as a result of mimicking its training data. A study from the University of Washington found that GPT-3 produced toxic language at a toxicity level comparable to the similar natural language processing models of GPT-2 and CTRL. OpenAI has implemented several strategies to limit the amount of toxic language generated by GPT-3. As a result, GPT-3 produced less toxic language compared to its predecessor model, GPT-1, although it produced both more generations and a higher toxicity of toxic language compared to CTRL Wiki, a language model trained entirely on Wikipedia data.[16]\n",
      " On June 11, 2020, OpenAI announced that users could request access to its user-friendly GPT-3 API‚Äîa \"machine learning toolset\"‚Äîto help OpenAI \"explore the strengths and limits\" of this new technology.[17][18] The invitation described how this API had a general-purpose \"text in, text out\" interface that can complete almost \"any English language task\", instead of the usual single use-case.[17] According to one user, who had access to a private early release of the OpenAI GPT-3 API, GPT-3 was \"eerily good\" at writing \"amazingly coherent text\" with only a few simple prompts.[19] In an initial experiment 80 US subjects were asked to judge if short ~200 word articles were written by humans or GPT-3. The participants judged correctly 52% of the time, doing only slightly better than random guessing.[1]\n",
      " On November 18, 2021, OpenAI announced that enough safeguards had been implemented that access to its API would be unrestricted.[20] OpenAI provided developers with a content moderation tool that helps them abide by OpenAI's content policy.[21] On January 27, 2022, OpenAI announced that its newest GPT-3 language models, collectively referred to as InstructGPT, was now the default language model used on their API. According to OpenAI, InstructGPT produced content that was better aligned to user intentions by following instructions better, generating fewer made-up facts, and producing somewhat less toxic content.[22]\n",
      " Because GPT-3 can \"generate news articles which human evaluators have difficulty distinguishing from articles written by humans,\"[5] GPT-3 has the \"potential to advance both the beneficial and harmful applications of language models.\"[1]:‚Ää34‚Ää In their May 28, 2020 paper, the researchers described in detail the potential \"harmful effects of GPT-3\"[5] which include \"misinformation, spam, phishing, abuse of legal and governmental processes, fraudulent academic essay writing and social engineering pretexting\".[1] The authors draw attention to these dangers to call for research on risk mitigation.[1][23]:‚Ää34‚Ää\n",
      " GPT-3 is capable of performing zero-shot, few-shot and one-shot learning.[1]\n",
      " In June 2022, Almira Osmanovic Thunstr√∂m wrote that GPT-3 was the primary author on an article on itself, that they had submitted it for publication,[24] and that it had been pre-published while waiting for completion of its review.[25]\n",
      " On March 15, 2022, OpenAI made available new versions of GPT-3 and Codex in its API with edit and insert capabilities under the names \"text-davinci-003\" and \"code-davinci-002\".[26] These models were described as more capable than previous versions and were trained on data up to June 2021.[27] On November 30, 2022, OpenAI began referring to these models as belonging to the \"GPT-3.5\" series,[28] and released ChatGPT, which was fine-tuned from a model in the GPT-3.5 series.[29]\n",
      " GPT-3's builder, OpenAI, was initially founded as a non-profit in 2015.[52] In 2019, OpenAI did not publicly release GPT-3's precursor model, breaking from OpenAI's previous open-source practices, citing concerns that the model would perpetuate fake news. OpenAI eventually released a version of GPT-2 that was 8% of the original model's size.[53] In the same year, OpenAI restructured to be a for-profit company.[54] In 2020, Microsoft announced the company had exclusive licensing of GPT-3 for Microsoft's products and services following a multi-billion dollar investment in OpenAI. The agreement permits OpenAI to offer a public-facing API such that users can send text to GPT-3 to receive the model's output, but only Microsoft will have access to GPT-3's source code.[8]\n",
      " Large language models, such as GPT-3, have come under criticism from a few of Google's AI ethics researchers for the environmental impact of training and storing the models, detailed in a paper co-authored by Timnit Gebru and Emily M. Bender in 2021.[55]\n",
      " The growing[when?] use of automated writing technologies based on GPT-3 and other language generators, has raised concerns regarding academic integrity[56] and raised the stakes of how universities and schools will gauge what constitutes academic misconduct such as plagiarism.[57]\n",
      " GPT was built with data from the Common Crawl dataset, a conglomerate of copyrighted articles, internet posts, web pages, and books scraped from 60 million domains over a period of 12 years. TechCrunch reports this training data includes copyrighted material from the BBC, The New York Times, Reddit, the full text of online books, and more.[58] In its response to a 2019 Request for Comments on Intellectual Property Protection for Artificial Intelligence Innovation from the United States Patent and Trademark Office (USPTO), OpenAI argued that \"Under current law, training AI systems [such as its GPT models] constitutes fair use,\" but that \"given the lack of case law on point, OpenAI and other AI developers like us face substantial legal uncertainty and compliance costs.\"[59]\n"
     ]
    }
   ],
   "source": [
    "text_content = parser.get_content()\n",
    "print(text_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ad809bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# chunks = parser.split_text(text_content, 4000*2)\n",
    "# len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a8c06f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_content = parser.get_content_slice('Today we‚Äôre excited', 'On June 11, 2018')\n",
    "# print(text_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e93f778a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt, tokens = prompts.extract_model_card(model_card.ATTRIBUTES, text_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "80b55a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = openai.text(\n",
    "    prompt=prompt, \n",
    "    max_tokens=tokens, \n",
    "    temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "edb58b7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```\n",
      "{\n",
      "    \"Name\": \"Generative Pre-trained Transformer 3 (GPT-3)\",\n",
      "    \"Size\": \"175 billion parameters, requiring 800GB to store\",\n",
      "    \"License\": \"Microsoft has exclusive licensing of GPT-3 for Microsoft's products and services\",\n",
      "    \"Publisher\": \"OpenAI\",\n",
      "    \"Description\": \"Autoregressive language model released in 2020 that uses deep learning to produce human-like text\",\n",
      "    \"Keywords\": \"Deep learning, Natural language processing (NLP), Transformer, Generative pre-training\",\n",
      "    \"Version\": \"GPT-3.5\",\n",
      "    \"Release Date\": \"May 28, 2020\",\n",
      "    \"Training corpus\": \"Common Crawl, WebText2, Books1, Books2, Wikipedia\",\n",
      "    \"Training method\": \"Generative pre-training\",\n",
      "    \"Evaluation method\": \"Human evaluators\",\n",
      "    \"Use Cases\": \"Text generation, summarizing texts, answering questions, coding in CSS, JSX, and Python\",\n",
      "    \"Compute\": \"800GB\",\n",
      "    \"Features\": \"Zero-shot, few-shot and one-shot learning, edit and insert capabilities\",\n",
      "    \"Limitations\": \"Potential to perpetuate fake news, environmental impact of training and storing the models, potential for misuse\",\n",
      "    \"Strengths\": \"Eerily good at writing amazingly coherent text, improved language understanding performances in natural language processing (NLP)\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "model_card_extract = response.choices[0].text.strip()\n",
    "print(model_card_extract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8d1f9f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_card.set_extract(model_card_extract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f8e39c7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Name': 'Generative Pre-trained Transformer 3 (GPT-3)', 'Size': '175 billion parameters, requiring 800GB to store', 'License': \"Microsoft has exclusive licensing of GPT-3 for Microsoft's products and services\", 'Publisher': 'OpenAI', 'Description': 'Autoregressive language model released in 2020 that uses deep learning to produce human-like text', 'Keywords': 'Deep learning, Natural language processing (NLP), Transformer, Generative pre-training', 'Version': 'GPT-3.5', 'Release Date': 'May 28, 2020', 'Training corpus': 'Common Crawl, WebText2, Books1, Books2, Wikipedia', 'Training method': 'Generative pre-training', 'Evaluation method': 'Human evaluators', 'Use Cases': 'Text generation, summarizing texts, answering questions, coding in CSS, JSX, and Python', 'Compute': '800GB', 'Features': 'Zero-shot, few-shot and one-shot learning, edit and insert capabilities', 'Limitations': 'Potential to perpetuate fake news, environmental impact of training and storing the models, potential for misuse', 'Strengths': 'Eerily good at writing amazingly coherent text, improved language understanding performances in natural language processing (NLP)'}\n"
     ]
    }
   ],
   "source": [
    "print(model_card.extract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7fc2d213",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt, tokens = prompts.summarize(text_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "962302af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-3 is a deep learning language model released in 2020 that uses generative pre-training to produce human-like text. It is the third-generation language prediction model in the GPT series, successor to GPT-2 created by OpenAI. GPT-3 has a capacity of 175 billion parameters and is capable of performing zero-shot, few-shot and one-shot learning. It can generate text that is difficult to distinguish from human-written text, and has potential applications in both beneficial and harmful applications. Microsoft has exclusive licensing of GPT-3, while OpenAI provides a public-facing API. There are concerns about the environmental impact of training and storing the model, as well as potential issues with academic integrity.\n"
     ]
    }
   ],
   "source": [
    "response = openai.text(\n",
    "    prompt=prompt, \n",
    "    max_tokens=tokens, \n",
    "    temperature=0)\n",
    "summary = response.choices[0].text.strip()\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3b13b48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_card.set_summary(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e6d860fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "title: Generative Pre-trained Transformer 3 (GPT-3)\n",
      "description: Autoregressive language model released in 2020 that uses deep learning to produce human-like text\n",
      "tags:\n",
      "  - Deep learning\n",
      "  - Natural language processing (NLP)\n",
      "  - Transformer\n",
      "  - Generative pre-training\n",
      "---\n",
      "\n",
      "# Generative Pre-trained Transformer 3 (GPT-3)\n",
      "\n",
      "**Autoregressive language model released in 2020 that uses deep learning to produce human-like text**\n",
      "\n",
      "| Publisher | License | Version | Release |\n",
      "| --- | --- | --- | --- |\n",
      "| OpenAI | Microsoft has exclusive licensing of GPT-3 for Microsoft's products and services | GPT-3.5 | May 28, 2020 |\n",
      "\n",
      "## Model Summary\n",
      "\n",
      "GPT-3 is a deep learning language model released in 2020 that uses generative pre-training to produce human-like text. It is the third-generation language prediction model in the GPT series, successor to GPT-2 created by OpenAI. GPT-3 has a capacity of 175 billion parameters and is capable of performing zero-shot, few-shot and one-shot learning. It can generate text that is difficult to distinguish from human-written text, and has potential applications in both beneficial and harmful applications. Microsoft has exclusive licensing of GPT-3, while OpenAI provides a public-facing API. There are concerns about the environmental impact of training and storing the model, as well as potential issues with academic integrity.\n",
      "\n",
      "## Model Resources\n",
      "\n",
      "[üìÑ Research Paper](https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf) | [üê± GitHub](https://github.com/openai/gpt-3) | [üï∏Ô∏è Wikipedia](https://en.wikipedia.org/wiki/GPT-3)\n",
      "\n",
      "## Model Details\n",
      "\n",
      "**Size:** 175 billion parameters, requiring 800GB to store\n",
      "\n",
      "**Use Cases:** Text generation, summarizing texts, answering questions, coding in CSS, JSX, and Python\n",
      "\n",
      "**Training corpus:** Common Crawl, WebText2, Books1, Books2, Wikipedia\n",
      "\n",
      "**Training method:** Generative pre-training\n",
      "\n",
      "**Evaluation method:** Human evaluators\n",
      "\n",
      "**Compute:** 800GB\n",
      "\n",
      "**Features:** Zero-shot, few-shot and one-shot learning, edit and insert capabilities\n",
      "\n",
      "**Limitations:** Potential to perpetuate fake news, environmental impact of training and storing the models, potential for misuse\n",
      "\n",
      "**Strengths:** Eerily good at writing amazingly coherent text, improved language understanding performances in natural language processing (NLP)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generated_markdown = model_card.generate_material_mkdown()\n",
    "print(generated_markdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "119493fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Model card saved as ../models/generative-pretrained-transformer--gpt.md'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_card.save_markdown(generated_markdown, \n",
    "                         filename=model_card.extract['Name'], \n",
    "                         folder='../models/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
