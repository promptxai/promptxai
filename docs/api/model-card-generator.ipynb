{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "769230c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptxai import models, content, prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74482f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai = models.OpenAI()\n",
    "model_card = models.ModelCard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ec7c0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = \"https://huggingface.co/dalle-mini/dalle-mini\"\n",
    "# url = \"https://ai.googleblog.com/2022/01/lamda-towards-safe-grounded-and-high.html\"\n",
    "# url = \"https://ai.facebook.com/blog/large-language-model-llama-meta-ai/\"\n",
    "# url = \"https://huggingface.co/bert-base-uncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc08ff32",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_card.set_url('https://huggingface.co/bert-base-uncased', 'huggingface')\n",
    "model_card.set_url('https://github.com/google-research/bert', 'github')\n",
    "model_card.set_url('https://arxiv.org/abs/1810.04805', 'paper')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66197430",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = content.Webpage(model_card.get_url('huggingface'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b478f815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT base model (uncased) Pretrained model on English language using a masked language modeling (MLM) objective. It was introduced in\n",
      "this paper and first released in\n",
      "this repository. This model is uncased: it does not make a difference\n",
      "between english and English. Disclaimer: The team releasing BERT did not write a model card for this model so this model card has been written by\n",
      "the Hugging Face team. BERT is a transformers model pretrained on a large corpus of English data in a self-supervised fashion. This means it\n",
      "was pretrained on the raw texts only, with no humans labeling them in any way (which is why it can use lots of\n",
      "publicly available data) with an automatic process to generate inputs and labels from those texts. More precisely, it\n",
      "was pretrained with two objectives: This way, the model learns an inner representation of the English language that can then be used to extract features\n",
      "useful for downstream tasks: if you have a dataset of labeled sentences, for instance, you can train a standard\n",
      "classifier using the features produced by the BERT model as inputs. BERT has originally been released in base and large variations, for cased and uncased input text. The uncased models also strips out an accent markers.Chinese and multilingual uncased and cased versions followed shortly after.Modified preprocessing with whole word masking has replaced subpiece masking in a following work, with the release of two models.Other 24 smaller models are released afterward.   The detailed release history can be found on the google-research/bert readme on github. You can use the raw model for either masked language modeling or next sentence prediction, but it's mostly intended to\n",
      "be fine-tuned on a downstream task. See the model hub to look for\n",
      "fine-tuned versions of a task that interests you. Note that this model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked)\n",
      "to make decisions, such as sequence classification, token classification or question answering. For tasks such as text\n",
      "generation you should look at model like GPT2. You can use this model directly with a pipeline for masked language modeling: Here is how to use this model to get the features of a given text in PyTorch: and in TensorFlow: Even if the training data used for this model could be characterized as fairly neutral, this model can have biased\n",
      "predictions: This bias will also affect all fine-tuned versions of this model. The BERT model was pretrained on BookCorpus, a dataset consisting of 11,038\n",
      "unpublished books and English Wikipedia (excluding lists, tables and\n",
      "headers). The texts are lowercased and tokenized using WordPiece and a vocabulary size of 30,000. The inputs of the model are\n",
      "then of the form: With probability 0.5, sentence A and sentence B correspond to two consecutive sentences in the original corpus, and in\n",
      "the other cases, it's another random sentence in the corpus. Note that what is considered a sentence here is a\n",
      "consecutive span of text usually longer than a single sentence. The only constrain is that the result with the two\n",
      "\"sentences\" has a combined length of less than 512 tokens. The details of the masking procedure for each sentence are the following: The model was trained on 4 cloud TPUs in Pod configuration (16 TPU chips total) for one million steps with a batch size\n",
      "of 256. The sequence length was limited to 128 tokens for 90% of the steps and 512 for the remaining 10%. The optimizer\n",
      "used is Adam with a learning rate of 1e-4, Œ≤1=0.9\\beta_{1} = 0.9Œ≤1‚Äã=0.9 and Œ≤2=0.999\\beta_{2} = 0.999Œ≤2‚Äã=0.999, a weight decay of 0.01,\n",
      "learning rate warmup for 10,000 steps and linear decay of the learning rate after. When fine-tuned on downstream tasks, this model achieves the following results: Glue test results:\n"
     ]
    }
   ],
   "source": [
    "text_content = parser.get_content()\n",
    "print(text_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4b81c4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#text_content = parser.get_content_slice('Today we‚Äôre excited', 'We\\'d to like to thank everyone').strip()\n",
    "#print(text_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "688299a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Match the attributes with values from text. Where values do not exist, set value to Unknown.\\n        \\n        Attributes:\\n        Model parameters\\n        Limitations of the model\\n        Strengths of the model\\n        Name of the model\\n        Company releasing the model\\n        Unique features of the model\\n        Comparison with similar models\\n        Use cases of the model\\n        Date of model release\\n        Compute infrastructure required for model\\n        Model license\\n        Model version\\n        Model description\\n        Model keywords\\n        Model task\\n        Model training data\\n        Model evaluation data\\n        Model hyperparameters\\n        Model training procedure\\n        Model evaluation procedure\\n\\n        Text: \\n        BERT base model (uncased) Pretrained model on English language using a masked language modeling (MLM) objective. It was introduced in\\nthis paper and first released in\\nthis repository. This model is uncased: it does not make a difference\\nbetween english and English. Disclaimer: The team releasing BERT did not write a model card for this model so this model card has been written by\\nthe Hugging Face team. BERT is a transformers model pretrained on a large corpus of English data in a self-supervised fashion. This means it\\nwas pretrained on the raw texts only, with no humans labeling them in any way (which is why it can use lots of\\npublicly available data) with an automatic process to generate inputs and labels from those texts. More precisely, it\\nwas pretrained with two objectives: This way, the model learns an inner representation of the English language that can then be used to extract features\\nuseful for downstream tasks: if you have a dataset of labeled sentences, for instance, you can train a standard\\nclassifier using the features produced by the BERT model as inputs. BERT has originally been released in base and large variations, for cased and uncased input text. The uncased models also strips out an accent markers.Chinese and multilingual uncased and cased versions followed shortly after.Modified preprocessing with whole word masking has replaced subpiece masking in a following work, with the release of two models.Other 24 smaller models are released afterward.   The detailed release history can be found on the google-research/bert readme on github. You can use the raw model for either masked language modeling or next sentence prediction, but it\\'s mostly intended to\\nbe fine-tuned on a downstream task. See the model hub to look for\\nfine-tuned versions of a task that interests you. Note that this model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked)\\nto make decisions, such as sequence classification, token classification or question answering. For tasks such as text\\ngeneration you should look at model like GPT2. You can use this model directly with a pipeline for masked language modeling: Here is how to use this model to get the features of a given text in PyTorch: and in TensorFlow: Even if the training data used for this model could be characterized as fairly neutral, this model can have biased\\npredictions: This bias will also affect all fine-tuned versions of this model. The BERT model was pretrained on BookCorpus, a dataset consisting of 11,038\\nunpublished books and English Wikipedia (excluding lists, tables and\\nheaders). The texts are lowercased and tokenized using WordPiece and a vocabulary size of 30,000. The inputs of the model are\\nthen of the form: With probability 0.5, sentence A and sentence B correspond to two consecutive sentences in the original corpus, and in\\nthe other cases, it\\'s another random sentence in the corpus. Note that what is considered a sentence here is a\\nconsecutive span of text usually longer than a single sentence. The only constrain is that the result with the two\\n\"sentences\" has a combined length of less than 512 tokens. The details of the masking procedure for each sentence are the following: The model was trained on 4 cloud TPUs in Pod configuration (16 TPU chips total) for one million steps with a batch size\\nof 256. The sequence length was limited to 128 tokens for 90% of the steps and 512 for the remaining 10%. The optimizer\\nused is Adam with a learning rate of 1e-4, Œ≤1=0.9\\\\beta_{1} = 0.9Œ≤1\\u200b=0.9 and Œ≤2=0.999\\\\beta_{2} = 0.999Œ≤2\\u200b=0.999, a weight decay of 0.01,\\nlearning rate warmup for 10,000 steps and linear decay of the learning rate after. When fine-tuned on downstream tasks, this model achieves the following results: Glue test results:'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt, tokens = prompts.extract_model_card(text_content)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86659369",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = openai.text(\n",
    "    prompt=prompt, \n",
    "    max_tokens=tokens, \n",
    "    temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55cbed70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session cost: $ 0.043440000000000006  Current run tokens:  {\n",
      "  \"completion_tokens\": 476,\n",
      "  \"prompt_tokens\": 1696,\n",
      "  \"total_tokens\": 2172\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "#print('Session cost: $', openai.get_session_cost(), ' Current run tokens: ', openai.get_tokens_usage())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "825cecfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: Unknown\n",
      "Limitations of the model: Bias will also affect all fine-tuned versions of this model\n",
      "Strengths of the model: Can use lots of publicly available data, pretrained on a large corpus of English data in a self-supervised fashion\n",
      "Name of the model: BERT base model (uncased)\n",
      "Company releasing the model: Google\n",
      "Unique features of the model: Modified preprocessing with whole word masking has replaced subpiece masking\n",
      "Comparison with similar models: GPT2\n",
      "Use cases of the model: Sequence classification, token classification or question answering\n",
      "Date of model release: This paper and first released in this repository\n",
      "Compute infrastructure required for model: 4 cloud TPUs in Pod configuration (16 TPU chips total)\n",
      "Model license: Unknown\n",
      "Model version: Unknown\n",
      "Model description: Pretrained model on English language using a masked language modeling (MLM) objective\n",
      "Model keywords: Masked language modeling, MLM, English language\n",
      "Model task: Masked language modeling or next sentence prediction\n",
      "Model training data: BookCorpus, a dataset consisting of 11,038 unpublished books and English Wikipedia (excluding lists, tables and headers)\n",
      "Model evaluation data: Glue test results\n",
      "Model hyperparameters: Adam with a learning rate of 1e-4, Œ≤1=0.9\\beta_{1} = 0.9Œ≤1‚Äã=0.9 and Œ≤2=0.999\\beta_{2} = 0.999Œ≤2‚Äã=0.999, a weight decay of 0.01, learning rate warmup for 10,000 steps and linear decay of the learning rate after\n",
      "Model training procedure: Pretrained with two objectives\n",
      "Model evaluation procedure: Fine-tuned on downstream tasks\n"
     ]
    }
   ],
   "source": [
    "model_card_extract = response.choices[0].text.strip()\n",
    "print(model_card_extract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4159a261",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_card.set_extract(model_card_extract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc90d959",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt, tokens = prompts.summarize(text_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70df701e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT is a transformers model pretrained on a large corpus of English data in a self-supervised fashion. It was introduced in a paper and first released in a repository. It is uncased, meaning it does not differentiate between English and english. It can be used for masked language modeling or next sentence prediction, but is mainly intended to be fine-tuned on downstream tasks such as sequence classification, token classification or question answering. It was trained on 4 cloud TPUs for one million steps with a batch size of 256 and a sequence length of 128 tokens for 90% of the steps and 512 for the remaining 10%. When fine-tuned on downstream tasks, it achieves good results on Glue test results.\n"
     ]
    }
   ],
   "source": [
    "response = openai.text(\n",
    "    prompt=prompt, \n",
    "    max_tokens=tokens, \n",
    "    temperature=0)\n",
    "summary = response.choices[0].text.strip()\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df96da9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_card.set_unknown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0903d4f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session cost: $ 0.07896  Current run tokens:  {\n",
      "  \"completion_tokens\": 200,\n",
      "  \"prompt_tokens\": 1576,\n",
      "  \"total_tokens\": 1776\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "#print('Session cost: $', openai.get_session_cost(), ' Current run tokens: ', openai.get_tokens_usage())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a7135de",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_card.set_summary(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f16182a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "title: BERT base model (uncased)\n",
      "description: Pretrained model on English language using a masked language modeling (MLM) objective\n",
      "tags:\n",
      "  - Masked language modeling\n",
      "  - MLM\n",
      "  - English language\n",
      "---\n",
      "\n",
      "# BERT base model (uncased)\n",
      "\n",
      "**Pretrained model on English language using a masked language modeling (MLM) objective**\n",
      "\n",
      "| Model Provider | Model License | Model Version | Model Release |\n",
      "| --- | --- | --- | --- |\n",
      "| Google | Unknown | Unknown | This paper and first released in this repository |\n",
      "\n",
      "## Model Summary\n",
      "\n",
      "BERT is a transformers model pretrained on a large corpus of English data in a self-supervised fashion. It was introduced in a paper and first released in a repository. It is uncased, meaning it does not differentiate between English and english. It can be used for masked language modeling or next sentence prediction, but is mainly intended to be fine-tuned on downstream tasks such as sequence classification, token classification or question answering. It was trained on 4 cloud TPUs for one million steps with a batch size of 256 and a sequence length of 128 tokens for 90% of the steps and 512 for the remaining 10%. When fine-tuned on downstream tasks, it achieves good results on Glue test results.\n",
      "\n",
      "[ü§ó Hugging Face](https://huggingface.co/bert-base-uncased) | [üê± Github](https://github.com/google-research/bert) | [üìÉ Paper](https://arxiv.org/abs/1810.04805)\n",
      "\n",
      "## Model Details\n",
      "\n",
      "**Task:** Masked language modeling or next sentence prediction\n",
      "\n",
      "**Model Parameters:** Unknown\n",
      "\n",
      "**Model Training Data:** BookCorpus, a dataset consisting of 11,038 unpublished books and English Wikipedia (excluding lists, tables and headers)\n",
      "\n",
      "**Model Evaluation Data:** Glue test results\n",
      "\n",
      "**Model Hyperparameters:** Adam with a learning rate of 1e-4, Œ≤1=0.9\\beta_{1} = 0.9Œ≤1‚Äã=0.9 and Œ≤2=0.999\\beta_{2} = 0.999Œ≤2‚Äã=0.999, a weight decay of 0.01, learning rate warmup for 10,000 steps and linear decay of the learning rate after\n",
      "\n",
      "**Model Training Procedure:** Pretrained with two objectives\n",
      "\n",
      "**Model Evaluation Procedure:** Fine-tuned on downstream tasks\n",
      "\n",
      "**Model Strengths:** Can use lots of publicly available data, pretrained on a large corpus of English data in a self-supervised fashion\n",
      "\n",
      "**Model Limitations:** Bias will also affect all fine-tuned versions of this model\n",
      "\n",
      "**Model Unique Features:** Modified preprocessing with whole word masking has replaced subpiece masking\n",
      "\n",
      "**Model Comparison with Similar Models:** GPT2\n",
      "\n",
      "**Model Use Cases:** Sequence classification, token classification or question answering\n",
      "\n",
      "**Model Compute Infrastructure Required:** 4 cloud TPUs in Pod configuration (16 TPU chips total)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generated_markdown = model_card.generate_material_mkdown()\n",
    "print(generated_markdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d93642b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Model card saved as ../models/bert-base-model-uncased.md'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_card.save_markdown(generated_markdown, \n",
    "                         filename=model_card.model_name, \n",
    "                         folder='../models/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
